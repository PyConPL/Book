\usemodule[pycon-2016]
\starttext

\Title{Uczenie maszynowe - czyli jak rozwiązywać nietrywialne problemy}
\Author{Jacek Śmietański}
\MakeTitlePage

\subsection[streszczenie]{Streszczenie}

Techniki uczenia maszynowego są szeroko wykorzystywane w różnych
obszarach codziennego życia. Problemy takie jak klasyfikacja czy
predykcja, przy rozwiązywaniu których nie sprawdzają się tradycyjne
algorytmy, mogą być z powodzeniem implementowane metodami pozwalającymi
nauczyć program podejmować trafne decyzje. Python idealnie nadaje się do
wykorzystania tych technik, w czym pomaga bez mała kilkadziesiąt mniej
lub bardziej wyspecjalizowanych bibliotek.

\subsection[rozwój-znaczenia-i-metod-uczenia-maszynowego]{Rozwój
znaczenia i metod uczenia maszynowego}

Gdy w roku 1965 na Uniwersytecie Stanforda powstawał system Dendral,
nikt pewnie nie przypuszczał, że ledwie 50 lat później uczenie maszynowe
(machine learning, ML) będzie techniką wszechobecną, z której korzystamy
codziennie, niemal na każdym kroku.

Dendral był specjalistycznym oprogramowaniem naukowym, służącym do
analizy, grupowania oraz rozpoznawania, nieznanych do tej pory
człowiekowi, molekuł związków chemicznych. Uzyskane przez program
rezultaty zostały opublikowane w~literaturze naukowej. Było to pierwsze
w historii odkrycie dokonane bezpośrednio przez komputer, a Dendral
uważany jest za prekursora systemów wykorzystujących uczenie maszynowe.

Motorem napędowym dynamicznego rozwoju technik sztucznej inteligencji
oraz uczenia maszynowego był wzrost ilości gromadzonych danych. Widać to
dobrze na przykład w dziedzinie biologii, gdzie pod koniec XX wieku
intensywnie badano sposoby pozyskiwania informacji o sekwencjach
nukleotydowych genów. W roku 2003 głośno było o zakończeniu wielkiego
projektu sekwencjonowania ludzkiego genomu (HGP, human genom project).
Uzyskanie pierwszej sekwencji całego genomu człowieka zajęło naukowcom
13 lat i kosztowało aż 3 mld dolarów. Dziś jesteśmy w stanie
zsekwencjonować cały genom w ciągu zaledwie kilku godzin, za kwotę rzędu
jedynie tysiąca dolarów. Czyste, surowe dane niewiele jednak nam dają.
Nie jest dla nas przecież ważna sekwencja sama w sobie, ale wynikające z
niej wnioski, takie jak określenie skłonności i ryzyka zachorowania na
poszczególne choroby, zidentyfikowanie oporności na niektóre leki (ważne
przy planowaniu skutecznego leczenia) itp.

Nawiasem mówiąc, koszt długotrwałego przechowywania danych sekwencyjnych
bywa na tyle duży, że często bardziej opłaca się zachować próbkę DNA i w
razie potrzeby ponownie przeprowadzić sekwencjonowanie. To kolejny ważny
aspekt związany z~„erą danych”, który, jako wykraczający poza tematykę
artykułu, pominę w~dalszych rozważaniach. Duża ilość gromadzonych danych
nie jest domeną jedynie biologii. Mamy obrazy medyczne, dane
meteorologiczne, dane gospodarcze, dane o ruchu w~sieci i szereg innych.
I podobnie -- bazując na tych pozornie bezużytecznych surowych danych,
chcemy wyciągnąć konkretne, praktyczne wnioski.

Zastosowania algorytmów uczenia maszynowego widzimy na co dzień. Kto z
nas wpisując hasło do popularnej wyszukiwarki nie zwrócił uwagi na
pojawiające się sugestie? Mamy podpowiedzi w pasku przeglądarki, po
witrynach oprowadzają nas wirtualni asystenci, w pisaniu tekstów pomaga
autokorekta, porządek w poczcie zapewniają wykrywacze spamu, dostajemy
propozycje artykułów podobnych do aktualnie czytanego, sugestie
podobnych tematycznie filmów, wyselekcjonowane reklamy. Uczenie
maszynowe pozwala na stworzenie szybkich i wiarygodnych prognoz pogody,
wspomaga ocenę zdolności kredytowej, prognozy ekonomiczne, obecne jest w
nawigacji samochodowej, grach komputerowych i wielu, wielu innych
aspektach naszego życia. Do bardziej zaawansowanych problemów należą
rozpoznawanie mowy, pisma odręcznego, interpretacja obrazów medycznych.

Uczenie maszynowe rozumiemy jako zdolność programu do uczenia się, do
generalizacji. Algorytm konstruuje się zwykle w ten sposób, że uruchamia
się program na niewielkim wycinku dostępnych danych, na podstawie
których dokonuje on parametryzacji, czyli inaczej mówiąc -- uczy się.
Jeżeli uczenie przebiegło pomyślnie, wówczas program będzie w stanie
podjąć właściwą decyzję nawet, jeżeli okażemy mu dane, z którymi nigdy
wcześniej nie miał do czynienia. Istotą problemów, które rozwiązujemy
metodami uczenia maszynowego, są klasyfikacja oraz predykcja. Nie jest
moim celem omówienie tutaj konkretnych technik, do których zaliczyć
możemy m.in. sztuczne sieci neuronowe (ANN, {\em artificial neural
networks}) będące (mocno uproszczoną) imitacją procesu uczenia
zachodzącego w mózgu, maszyny wektorów nośnych (SVM, {\em support vector
machines}), których idea polega na optymalizacji granicy decyzyjnej,
ukryte modele Markowa (HMM, {\em hidden Markov models}) mające
zastosowanie w analizie procesów stochastycznych, czy drzewa decyzyjne
(DT, {\em decision trees}), które dokonują klasyfikacji na podstawie
odpowiednio wygenerowaniej struktury drzewiastej. W ostatnich latach
furorę robi uczenie głębokie ({\em deep learning}), które jest swego
rodzaju rozszerzeniem i udoskonaleniem trochę już odchodzącej na boczny
tor idei sieci neuronowych. Do popularnych technik zaliczymy również
algorytm k-średnich. Czytelników zainteresowanych głębszym wyjaśnieniem
poszczególnych algorytmów odsyłam do jakże bogatej literatury, np.
{[}3{]}.

\subsection[najpopularniejsze-biblioteki-ml]{Najpopularniejsze
biblioteki ML}

Inspiracją do napisania tego artykułu było dla mnie znalezione w sieci
zestawienie 20 bibliotek ML dla Pythona {[}6{]} uporządkowanych według
dość oryginalnego kryterium popularności -- wyrażało się ono liczbą
commitów na githubie oraz osób biorących udział w tworzeniu biblioteki
(contributors). Liczba dostępnych bibliotek jest jednak znacznie
większa. Joseph Misiti w swoim repozytorium zgromadził zestawienie
listujące grubo ponad 150 różnych bibliotek dla Pythona, które w jakimś
stopniu są wykorzystywane w uczeniu maszynowym {[}5{]}. Niewątpliwie
najpopularniejszą biblioteką oraz de facto standardem jest
{\bf scikit-learn} {[}11{]}. Biblioteka ta, oparta na popularnych
pakietach numerycznych i naukowych NumPy oraz SciPy, cieszy się też
dużym wsparciem społeczności. Jest biblioteką ogólnego zastosowania --
zawiera wiele modułów implementujących różne algorytmy oraz techniki ML.
Biblioteka jest bardzo łatwa w użyciu i wydajna. Jednak za tę łatwość i
mnogość zastosowań płaci się elastycznością. Trudno w scikit-learn
zmodyfikować szczegółowe parametry algorytmu, dopasowując go do
konkretnego problemu. Dlatego, gdy chcemy wykonywać operacje bardziej
niskopoziomowe, warto sięgnąć po dedykowaną, bardziej elastyczną
bibliotekę, jak np. {\bf PyBrain} {[}12{]}, pozwalającą skonstruować
praktycznie dowolną sieć neuronową, modyfikując np. funkcje aktywacji
itp. PyBrain również wykorzystuje SciPy.

Istnieją też biblioteki, które rozszerzają funkcjonalność pakietu
scikit-learn, np. {\bf Nilearn} {[}13{]}, zawierająca funkcje
szczególnie przydatne w analizie obrazów aktywności mózgu.

Ponieważ metody uczenia maszynowego wymagają zwykle obliczeń na
macierzach i innych złożonych strukturach matematycznych, wiele
bibliotek bazuje na specjalistycznych bibliotekach numerycznych.
Wspomniany wyżej scikit-learn wymaga SciPy i NumPy. Inną biblioteką
implementującą zaawansowane obliczenia numeryczne jest {\bf Theano}
{[}14{]}. Theano może również być samodzielnie wykorzystywana do ML,
szczególnie w implementacji sieci neuronowych oraz uczenia głębokiego.
Theano implementuje również rozwiązania pozwalające efektywnie wykonywać
obliczenia na procesorach graficznych (GPU).

Wśród bibliotek bazujących na Theano warto wskazać {\bf Pylearn2}
{[}15{]} oraz {\bf Block} {[}16{]}, {\bf Keras} {[}17{]} i {\bf Lasagne}
{[}18{]}.

Ciekawym pakietem jest również {\bf Tensorflow} {[}19{]}, stworzony w
ramach projektu Google Brain. Implementuje wysokopoziomowy dostęp do
sieci neuronowych. Do niewątpliwych zalet zaliczyć tu należy obsługę
obliczeń równoległych oraz na procesorach graficznych. Biblioteka
napisana jest w większości w C++, ale posiada niezbędne wiązania do
Pythona.

Wśród bibliotek najłatwiej znajdziemy te wspierające sieci neuronowe czy
uczenie głębokie. Nie brakuje jednak też modułów pozwalających na użycie
mniej popularnych metod, np. {\bf Pyevolve} {[}20{]} jest przykładem
biblioteki implementującej algorytmy genetyczne. Z kolei {\bf NuPIC}
{[}21{]} implementuje metodę hierarchical temporal memory (HTM).
Technika HTM architekturą przypomina sieci neuronowe, ma jednak inne
podłoże biologiczne i zasadę działania.

{\bf Nltk} {[}22{]} jest biblioteką dedykowaną dla problemów
przetwarzania języka naturalnego.

Biblioteką ogólnego przeznaczenia, bez zależności od SciPy i Theano,
jest {\bf Pattern} {[}23{]}, dedykowana w szczególności do eksploracji
witryn internetowych. Wspomnieć należy również biblioteki {\bf H2O}
{[}24{]} i {\bf caffe} {[}25{]}.

Zwieńczeniem tej niezwykle skrótowej prezentacji niech będzie {\bf fuel}
{[}26{]}, biblioteka zapewniająca łatwy dostęp do popularnych zbiorów
danych takich jak MNIST (baza danych pisma odręcznego), CIFAR-10 (baza
obrazów) czy Google's One Billion Words (baza tekstowa). Z kolei
{\bf Python Machine Learning Samples} {[}27{]} zawiera zbiór
przykładowych aplikacji wykorzystujących ML.

Tabela 1. Wybrane biblioteki ML dla Pythona uporządkowane według liczby
współautorów (ang. contributors) {[}dane liczbowe według stanu z
31.08.2016{]}.

\placetable[none]{}
\starttable[|l|l|l|l|]
\HL
\NC Biblioteka
\NC commits
\NC contributors
\NC https://github.com/
\NC\AR
\HL
\NC scikit-learn
\NC 21148
\NC 661
\NC scikit-learn/scikit-learn
\NC\AR
\NC tensorflow
\NC 7339
\NC 372
\NC tensorflow/tensorflow
\NC\AR
\NC Theano
\NC 23286
\NC 250
\NC Theano/Theano
\NC\AR
\NC keras
\NC 2592
\NC 250
\NC fchollet/keras
\NC\AR
\NC caffe
\NC 3766
\NC 208
\NC BVLC/caffe
\NC\AR
\NC pylearn2
\NC 7100
\NC 116
\NC lisa-lab/pylearn2
\NC\AR
\NC NuPIC
\NC 5986
\NC 73
\NC numenta/nupic
\NC\AR
\NC H2O
\NC 19092
\NC 62
\NC h2oai/h2o-3
\NC\AR
\NC lasagne
\NC 1050
\NC 51
\NC Lasagne/Lasagne
\NC\AR
\NC blocks
\NC 3175
\NC 48
\NC mila-udem/blocks
\NC\AR
\NC nilearn
\NC 5145
\NC 45
\NC nilearn/nilearn
\NC\AR
\NC pybrain
\NC 984
\NC 31
\NC pybrain/pybrain
\NC\AR
\NC fuel
\NC 1035
\NC 28
\NC mila-udem/fuel
\NC\AR
\NC pattern
\NC 943
\NC 20
\NC clips/pattern
\NC\AR
\NC fann
\NC 156
\NC 19
\NC libfann/fann
\NC\AR
\NC machine-learning-samples
\NC 29
\NC 13
\NC awslabs/machine-learning-samples
\NC\AR
\NC Pyevolve
\NC 168
\NC 12
\NC perone/Pyevolve
\NC\AR
\HL
\stoptable

\subsection[bibliografia]{Bibliografia}

\startitemize[n,packed][stopper=.,width=2.0em]
\item
  Denny Britz. Implementing a Neural Network from Scratch in Python --
  An Introduction.
  http://www.wildml.com/2015/09/implementing-a-neural-network-from-scratch/
\item
  Piotr Górecki. Uczenie maszynowe, sztuczna inteligencja i
  (samo)świadomość. 2014. http://www.tabletowo.pl/2014/11/23/\crlf
  uczenie-maszynowe-sztuczna-inteligencja-i-samoswiadomosc/
\item
  Alex S. Holehouse. Stanford Machine Learning.
  http://www.holehouse.org/mlclass/
\item
  Matthew Mayo. 7 Steps to Mastering Machine Learning With Python. 2015.
  http://www.kdnuggets.com/2015/11/seven-steps-machine-learning-python.html
\item
  Joseph Misiti. Awesome Machine Learning.
  https://github.com/josephmisiti/\crlf awesome-machine-learning\#python-cv
\item
  Geethika Bhavya Peddibhotla. Top 20 Python Machine Learning Open
  Source Projects. 2015.
  http://www.kdnuggets.com/2015/06/top-20-python-machine-learning-open-source-projects.html
\item
  Python Tools for Machine Learning. 2014.
  https://www.cbinsights.com/blog/python-tools-machine-learning/
\item
  Scott Robinson. The Best Machine Learning Libraries in Python. 2015.
  http://stackabuse.com/the-best-machine-learning-libraries-in-python/
\item
  Adrian Rosebrock. My Top 9 Favorite Python Deep Learning Libraries.
  2016.
  https://www.pyimagesearch.com/2016/06/27/my-top-9-favorite-python-deep-learning-libraries/
\item
  Scikit-learn. Machine Learning in Python, http://scikit-learn.org/
\item
  https://github.com/scikit-learn/scikit-learn
\item
  https://github.com/pybrain/pybrain
\item
  https://github.com/nilearn/nilearn
\item
  https://github.com/Theano/Theano
\item
  https://github.com/lisa-lab/pylearn2
\item
  https://github.com/mila-udem/blocks
\item
  https://github.com/fchollet/keras
\item
  https://github.com/Lasagne/Lasagne
\item
  https://github.com/tensorflow/tensorflow
\item
  https://github.com/perone/Pyevolve
\item
  https://github.com/numenta/nupic
\item
  http://www.nltk.org/
\item
  https://github.com/clips/pattern
\item
  https://github.com/h2oai/h2o-3
\item
  https://github.com/BVLC/caffe
\item
  https://github.com/mila-udem/fuel
\item
  https://github.com/awslabs/machine-learning-samples
\stopitemize


\stoptext
